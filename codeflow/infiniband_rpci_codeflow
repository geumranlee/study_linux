##################################
# TBD
##################################
1. netlink send 함수는 rpci어디에 있는가?

static struct pernet_operations rdma_dev_net_ops = {
	.init = rdma_dev_init_net,
	.exit = rdma_dev_exit_net,
	.id = &rdma_dev_net_id,
	.size = sizeof(struct rdma_dev_net),
};

ib_core_init(void)
-> register_pernet_device(&rdma_dev_net_ops);

struct netlink_kernel_cfg cfg = {
	.input	= rdma_nl_rcv,
};

#define NETLINK_RDMA		20

net_ns_init()
-> setup_net()
->-> list_for_each_entry(ops, &pernet_list, list) ops_init(ops, net);
->->-> ops->init(net);
->->->-> rdma_dev_init_net()
->->->->-> rdma_nl_net_init()
->->->->->-> netlink_kernel_create(net, NETLINK_RDMA, &cfg);


-> rdma_nl_rcv()
->-> rdma_nl_rcv_skb()
->-> rdma_nl_rcv_msg()
->->-> cb_table = get_cb_table(skb, index, op)
->->-> cb_table[op].doit(skb, nlh, extack)
->->->-> nldev_newlink()
->->->->-> ops = link_ops_get(type);
->->->->-> ops->newlink(ibdev_name, ndev) -> rpci_newlink()
->->->->->-> rdev = ib_alloc_device(rpci_dev, ib_dev);
->->->->->->->
->->->->->-> rpci_init(rdev);
->->->->->->-> rpci_init_device_param(rdev)
->->->->->->-> rpci_init_ports(rdev)
->->->->->->-> rpci_init_pools(rdev)
->->->->->-> rpci_pci_init(rdev, inited_pdev);
->->->->->-> rdev->ndev = ndev;
->->->->->-> rpci_set_mtu(rdev, 0x400);
->->->->->-> rpci_register_device(rdev, ibdev_name)
->->->->->->-> set rpci->ib_dev's fields
->->->->->->-> ib_set_device_ops(dev, &rpci_dev_ops)
->->->->->->->-> ib_device->ops->{mmap,...} = rpci_dev_ops->{mmap,...}
->->->->->->-> ib_device_set_netdev(&rpci->ib_dev, rpci->ndev, 1)

static const struct ib_device_ops rpci_dev_ops = {
	.mmap = rpci_mmap,
};

static struct rdma_link_ops rpci_link_ops = {
	.type = "rpci",
	.newlink = rpci_newlink,
};

static struct pci_driver rpci_pci_driver = {
	.name		= DRV_NAME,
	.id_table	= rpci_pci_table,
	.probe		= rpci_pci_probe,
	.remove		= rpci_pci_remove,
};


rpci_module_init()
-> pci_register_driver(&rpci_pci_driver)
->-> __pci_register_driver(driver, THIS_MODULE, KBUILD_MODNAME)
->->-> driver_register();
-> rdma_link_register(&rpci_link_ops)
->-> list_add(&rpci_link_ops->list, &link_ops)

rpci_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
-> pci_enable_device(pdev)
-> rpci_alloc_bars(pdev);
-> pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
-> pci_set_master(pdev);
-> inited_pdev = pdev;

##################################
# /dev/infiniband/rdma_cm
##################################

static struct miscdevice ucma_misc = {
	.minor          = MISC_DYNAMIC_MINOR,
	.name           = "rdma_cm",
	.nodename       = "infiniband/rdma_cm",
	.mode           = 0666,
	.fops           = &ucma_fops,
};

static const struct file_operations ucma_fops = {
	.owner   = THIS_MODULE,
	.open    = ucma_open,
	.release = ucma_close,
	.write   = ucma_write,
	.poll    = ucma_poll,
	.llseek  = no_llseek,
};

static ssize_t (*ucma_cmd_table[])(struct ucma_file *file,
				   const char __user *inbuf,
				   int in_len, int out_len) = {
	[RDMA_USER_CM_CMD_CREATE_ID] 	 = ucma_create_id,
	[RDMA_USER_CM_CMD_DESTROY_ID]	 = ucma_destroy_id,
	[RDMA_USER_CM_CMD_BIND_IP]	 = ucma_bind_ip,
	[RDMA_USER_CM_CMD_RESOLVE_IP]	 = ucma_resolve_ip,
	[RDMA_USER_CM_CMD_RESOLVE_ROUTE] = ucma_resolve_route,
	[RDMA_USER_CM_CMD_QUERY_ROUTE]	 = ucma_query_route,
	[RDMA_USER_CM_CMD_CONNECT]	 = ucma_connect,
	[RDMA_USER_CM_CMD_LISTEN]	 = ucma_listen,
	[RDMA_USER_CM_CMD_ACCEPT]	 = ucma_accept,
	[RDMA_USER_CM_CMD_REJECT]	 = ucma_reject,
	[RDMA_USER_CM_CMD_DISCONNECT]	 = ucma_disconnect,
	[RDMA_USER_CM_CMD_INIT_QP_ATTR]	 = ucma_init_qp_attr,
	[RDMA_USER_CM_CMD_GET_EVENT]	 = ucma_get_event,
	[RDMA_USER_CM_CMD_GET_OPTION]	 = NULL,
	[RDMA_USER_CM_CMD_SET_OPTION]	 = ucma_set_option,
	[RDMA_USER_CM_CMD_NOTIFY]	 = ucma_notify,
	[RDMA_USER_CM_CMD_JOIN_IP_MCAST] = ucma_join_ip_multicast,
	[RDMA_USER_CM_CMD_LEAVE_MCAST]	 = ucma_leave_multicast,
	[RDMA_USER_CM_CMD_MIGRATE_ID]	 = ucma_migrate_id,
	[RDMA_USER_CM_CMD_QUERY]	 = ucma_query,
	[RDMA_USER_CM_CMD_BIND]		 = ucma_bind,
	[RDMA_USER_CM_CMD_RESOLVE_ADDR]	 = ucma_resolve_addr,
	[RDMA_USER_CM_CMD_JOIN_MCAST]	 = ucma_join_multicast
};

ucma_init()
> misc_register(&ucma_misc);
> device_create_file(ucma_misc.this_device, &dev_attr_abi_version);
> ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table);
> ib_register_client(&rdma_cma_client);
>> assign_client_id(client);
>> xa_for_each_marked (&devices, index, device, DEVICE_REGISTERED) add_client_context(device, client);

ucam_open()
> file = kmalloc(sizeof *file, GFP_KERNEL); // struct ucma_file *file
> file->close_wq = alloc_ordered_workqueue("ucma_close_id", WQ_MEM_RECLAIM);
> INIT_LIST_HEAD(&file->event_list); INIT_LIST_HEAD(&file->ctx_list); init_waitqueue_head(&file->poll_wait); mutex_init(&file->mut);
> filp->private_data = file; file->filp = filp;
> stream_open(inode, filp);

ucma_poll()
> struct ucma_file *file = filp->private_data;
> poll_wait(filp, &file->poll_wait, wait);
> if (!list_empty(&file->event_list)) mask = EPOLLIN | EPOLLRDNORM;
> return mask

static ssize_t ucma_write(struct file *filp, const char __user *buf, size_t len, loff_t *pos)
> struct ucma_file *file = filp->private_data;
> copy_from_user(&hdr, buf, sizeof(hdr))
> hdr.cmd = array_index_nospec(hdr.cmd, ARRAY_SIZE(ucma_cmd_table));
> ucma_cmd_table[hdr.cmd](file, buf + sizeof(hdr), hdr.in, hdr.out);

// ucma_bind when hdr.cmd is [RDMA_USER_CM_CMD_BIND]
static ssize_t ucma_bind(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx(file, cmd.id);
>> ctx = _ucma_find_context(id, file);
>>> ctx = xa_load(&ctx_table, id);
> rdma_bind_addr(ctx->cm_id, (struct sockaddr *) &cmd.addr);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> cma_check_linklocal(&id->route.addr.dev_addr, addr);
>>> sin6 = (struct sockaddr_in6 *) addr;
>>> dev_addr->bound_dev_if = sin6->sin6_scope_id;
>> memcpy(cma_src_addr(id_priv), addr, rdma_addr_size(addr));
>> daddr = cma_dst_addr(id_priv); daddr->sa_family = addr->sa_family;
>>> (struct sockaddr *) &id_priv->id.route.addr.dst_addr;
>> daddr->sa_family = addr->sa_family;
>> cma_get_port(id_priv);
>>> if (cma_family(id_priv) != AF_IB) ps = cma_select_inet_ps(id_priv);
>>> else ps = cma_select_ib_ps(id_priv);
>>> if (cma_any_port(cma_src_addr(id_priv))) >>>  cma_alloc_any_port(ps, id_priv);
>>> else cma_use_port(ps, id_priv);
>>>>>> case RDMA_PS_IB: return &pernet->ib_ps;
>>>>> return xa_load(xa, snum);

static ssize_t ucma_create_id(struct ucma_file *file, const char __user *inbuf,	int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ucma_get_qp_type(&cmd, &qp_type);
>> switch (cmd->ps) {
>> case RDMA_PS_TCP: *qp_type = IB_QPT_RC;
>> case RDMA_PS_{UDP,IPOIB}: *qp_type = IB_QPT_UD;
>> case RDMA_PS_IB: *qp_type = cmd->qp_type;
> ctx = ucma_alloc_ctx(file);
>> ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
>> ctx->file = file;
>> xa_alloc(&ctx_table, &ctx->id, ctx, xa_limit_32b, GFP_KERNEL)
>> list_add_tail(&ctx->list, &file->ctx_list);
> ctx->uid = cmd.uid;
> cm_id = __rdma_create_id(current->nsproxy->net_ns, ucma_event_handler, ctx, cmd.ps, qp_type, NULL);
>> kzalloc and setting id_priv
> resp.id = ctx->id;
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))
> ctx->cm_id = cm_id;

static ssize_t ucma_query(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> if (copy_from_user(&cmd, inbuf, sizeof(cmd)))
> response = u64_to_user_ptr(cmd.response);
> ctx = ucma_get_ctx(file, cmd.id);
> switch (cmd.option) {
> case RDMA_USER_CM_QUERY_ADDR: ret = ucma_query_addr(ctx, response, out_len);
> case RDMA_USER_CM_QUERY_PATH: ret = ucma_query_path(ctx, response, out_len);
> case RDMA_USER_CM_QUERY_GID: ret = ucma_query_gid(ctx, response, out_len);
> ucma_put_ctx(ctx);

static ssize_t ucma_destroy_id(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = _ucma_find_context(cmd.id, file);
> ctx->destroying = 1;
> flush_workqueue(ctx->file->close_wq);
> if (!ctx->closing) {
>  wait_for_completion(&ctx->comp);
tbd
>  rdma_destroy_id(ctx->cm_id);}
> resp.events_reported = ucma_free_ctx(ctx);
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))

static ssize_t ucma_resolve_addr(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> if (copy_from_user(&cmd, inbuf, sizeof(cmd)))
> ctx = ucma_get_ctx(file, cmd.id);
>> ctx = _ucma_find_context(id, file);
>>> ctx = xa_load(&ctx_table, id);
> rdma_resolve_addr(ctx->cm_id, (struct sockaddr *) &cmd.src_addr, (struct sockaddr *) &cmd.dst_addr, cmd.timeout_ms);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> memcpy(cma_dst_addr(id_priv), dst_addr, rdma_addr_size(dst_addr));
>> if (id_priv->state == RDMA_CM_IDLE) {
>>  cma_bind_addr(id, src_addr, dst_addr);
>>> if (!src_addr || !src_addr->sa_family) {
>>>  src_addr = (struct sockaddr *) &id->route.addr.src_addr;
>>>  src_addr->sa_family = dst_addr->sa_family;
>>>  if (IS_ENABLED(CONFIG_IPV6) && dst_addr->sa_family == AF_INET6) {
>>>   struct sockaddr_in6 *src_addr6 = (struct sockaddr_in6 *) src_addr;
>>>   struct sockaddr_in6 *dst_addr6 = (struct sockaddr_in6 *) dst_addr;
>>>   src_addr6->sin6_scope_id = dst_addr6->sin6_scope_id;
>>>   if (ipv6_addr_type(&dst_addr6->sin6_addr) & IPV6_ADDR_LINKLOCAL) id->route.addr.dev_addr.bound_dev_if = dst_addr6->sin6_scope_id;
>>>   else if (dst_addr->sa_family == AF_IB) ((struct sockaddr_ib *) src_addr)->sib_pkey = ((struct sockaddr_ib *) dst_addr)->sib_pkey;
>>>  }
>>> }
>>> return rdma_bind_addr(id, src_addr);
>>  if (ret) memset(cma_dst_addr(id_priv), 0, rdma_addr_size(dst_addr));
>> }
>> if (cma_any_addr(dst_addr)) cma_resolve_loopback(id_priv);
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> if (!id_priv->cma_dev) cma_bind_loopback(id_priv);
>>>> cma_dev = NULL;
>>>> list_for_each_entry(cur_dev, &dev_list, list) {
>>>>  if (cma_family(id_priv) == AF_IB && !rdma_cap_ib_cm(cur_dev->device, 1)) continue;
>>>>  if (!cma_dev) cma_dev = cur_dev;
>>>>  for (p = 1; p <= cur_dev->device->phys_port_cnt; ++p) {
>>>>   if (!ib_get_cached_port_state(cur_dev->device, p, &port_state)
>>>>     && port_state == IB_PORT_ACTIVE) {
>>>>> 	  *port_state = device->port_data[port_num].cache.port_state;
>>>> 	cma_dev = cur_dev;
>>>> 	goto port_found;
>>>>   }
>>>>  }
>>>> }
>>>> p = 1;
>>>> port_found:
>>>> rdma_query_gid(cma_dev->device, p, 0, &gid);
>>>>> table = rdma_gid_table(device, port_num);
>>>>>> return device->port_data[port].cache.gid;
>>>>> memcpy(gid, &table->data_vec[index]->attr.gid, sizeof(*gid));
>>>> ib_get_cached_pkey(cma_dev->device, p, 0, &pkey);
>>>>> read_lock_irqsave(&device->cache.lock, flags);
>>>>> cache = device->port_data[port_num].cache.pkey;
>>>>> *pkey = cache->table[index];
>>>>> EXPORT_SYMBOL(ib_get_cached_pkey);
>>>> id_priv->id.route.addr.dev_addr.dev_type = (rdma_protocol_ib(cma_dev->device, p)) ? ARPHRD_INFINIBAND : ARPHRD_ETHER;
>>>> rdma_addr_set_sgid(&id_priv->id.route.addr.dev_addr, &gid);
>>>>>  memcpy(dev_addr->src_dev_addr + rdma_addr_gid_offset(dev_addr), gid, sizeof *gid);
>>>> ib_addr_set_pkey(&id_priv->id.route.addr.dev_addr, pkey);
>>>>> dev_addr->broadcast[8] = pkey >> 8;
>>>>> dev_addr->broadcast[9] = (unsigned char) pkey;
>>>> id_priv->id.port_num = p;
>>>> cma_attach_to_dev(id_priv, cma_dev);
>>>>> _cma_attach_to_dev(id_priv, cma_dev);
>>>>>> cma_ref_dev(cma_dev);
>>>>>> id_priv->cma_dev = cma_dev;
>>>>>> id_priv->id.device = cma_dev->device;
>>>>>> id_priv->id.route.addr.dev_addr.transport = rdma_node_get_transport(cma_dev->device->node_type);
>>>>>> list_add_tail(&id_priv->list, &cma_dev->id_list);
>>>>>> if (id_priv->res.kern_name) rdma_restrack_kadd(&id_priv->res);
>>>>>>> set_kern_name(res);
>>>>>>> rdma_restrack_add(res);
>>>>>>>> struct ib_device *dev = res_to_dev(res);
>>>>>>>> rt = &dev->res[res->type];
>>>>>>>> kref_init(&res->kref);
>>>>>>>> init_completion(&res->comp);
>>>>>>>> if (res->type == RDMA_RESTRACK_QP) {
>>>>>>>>  struct ib_qp *qp = container_of(res, struct ib_qp, res);
>>>>>>>>  ret = xa_insert(&rt->xa, qp->qp_num, res, GFP_KERNEL);
>>>>>>>>  res->id = ret ? 0 : qp->qp_num;
>>>>>>>> } else if (res->type == RDMA_RESTRACK_COUNTER) {
>>>>>>>>  struct rdma_counter *counter;
>>>>>>>>  counter = container_of(res, struct rdma_counter, res);
>>>>>>>>  ret = xa_insert(&rt->xa, counter->id, res, GFP_KERNEL);
>>>>>>>>  res->id = ret ? 0 : counter->id;
>>>>>>>> } else ret = xa_alloc_cyclic(&rt->xa, &res->id, res, xa_limit_32b, &rt->next_id, GFP_KERNEL);
>>>>>>> EXPORT_SYMBOL(rdma_restrack_kadd);
>>>>>> else rdma_restrack_uadd(&id_priv->res);
>>>>>>> rdma_restrack_add(res);
>>>>>>> EXPORT_SYMBOL(rdma_restrack_uadd);
>>>>> id_priv->gid_type = cma_dev->default_gid_type[id_priv->id.port_num - rdma_start_port(cma_dev->device)];
>>>> cma_set_loopback(cma_src_addr(id_priv));
>>>>> switch (addr->sa_family)
>>>>> case AF_INET: ((struct sockaddr_in *) addr)->sin_addr.s_addr = htonl(INADDR_LOOPBACK);
>>>>> case AF_INET6: ipv6_addr_set(&((struct sockaddr_in6 *) addr)->sin6_addr, 0, 0, 0, htonl(1));
>>>>> default: ib_addr_set(&((struct sockaddr_ib *) addr)->sib_addr, 0, 0, 0, htonl(1));
>>> rdma_addr_get_sgid(&id_priv->id.route.addr.dev_addr, &gid);
>>>> dev_addr->broadcast[8] = pkey >> 8;
>>>> dev_addr->broadcast[9] = (unsigned char) pkey;
>>> rdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, &gid);
>>>> memcpy(dev_addr->dst_dev_addr + rdma_addr_gid_offset(dev_addr), gid, sizeof *gid);
>>> cma_init_resolve_addr_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work->old_state = RDMA_CM_ADDR_QUERY;
>>>> work->new_state = RDMA_CM_ADDR_RESOLVED;
>>>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
>>> queue_work(cma_wq, &work->work);
>> else {
>>  if (dst_addr->sa_family == AF_IB) cma_resolve_ib_addr(id_priv);
>>>  work = kzalloc(sizeof *work, GFP_KERNEL);
>>>  if (!id_priv->cma_dev) ret = cma_resolve_ib_dev(id_priv);
>>>> addr = (struct sockaddr_ib *) cma_dst_addr(id_priv);
>>>> dgid = (union ib_gid *) &addr->sib_addr;
>>>> pkey = ntohs(addr->sib_pkey);
>>>> list_for_each_entry(cur_dev, &dev_list, list) {
>>>>  for (p = 1; p <= cur_dev->device->phys_port_cnt; ++p) {
>>>>   if (!rdma_cap_af_ib(cur_dev->device, p)) continue;
>>>>   if (ib_find_cached_pkey(cur_dev->device, p, pkey, &index)) continue;
>>>>   if (ib_get_cached_port_state(cur_dev->device, p, &port_state)) continue;
>>>>   for (i = 0; !rdma_query_gid(cur_dev->device, p, i, &gid); i++) {
>>>>    if (!memcmp(&gid, dgid, sizeof(gid))) cma_dev = cur_dev; sgid = gid; id_priv->id.port_num = p; goto found;
>>>>    if (!cma_dev && (gid.global.subnet_prefix == dgid->global.subnet_prefix) && port_state == IB_PORT_ACTIVE) {
>>>>     cma_dev = cur_dev;
>>>>     sgid = gid;
>>>>     id_priv->id.port_num = p;
>>>>     goto found;
>>>>    }
>>>>   }
>>>>  }
>>>> }
>>>> found:
>>>> cma_attach_to_dev(id_priv, cma_dev);
>>>> addr = (struct sockaddr_ib *)cma_src_addr(id_priv);
>>>> memcpy(&addr->sib_addr, &sgid, sizeof(sgid));
>>>> cma_translate_ib(addr, &id_priv->id.route.addr.dev_addr);
>>>>> dev_addr->dev_type = ARPHRD_INFINIBAND;
>>>>> rdma_addr_set_sgid(dev_addr, (union ib_gid *) &sib->sib_addr);
>>>>> ib_addr_set_pkey(dev_addr, ntohs(sib->sib_pkey));
>>>  rdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, (union ib_gid *)&(((struct sockaddr_ib *) &id_priv->id.route.addr.dst_addr)->sib_addr));
>>>>  memcpy(dev_addr->dst_dev_addr + rdma_addr_gid_offset(dev_addr), gid, sizeof *gid);
>>>  cma_init_resolve_addr_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work->old_state = RDMA_CM_ADDR_QUERY;
>>>> work->new_state = RDMA_CM_ADDR_RESOLVED;
>>>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
>>>  queue_work(cma_wq, &work->work);
>>  else rdma_resolve_ip(cma_src_addr(id_priv), dst_addr, &id->route.addr.dev_addr, timeout_ms, addr_handler, false, id_priv);
>>>  req = kzalloc(sizeof *req, GFP_KERNEL);
>>>  src_in = (struct sockaddr *) &req->src_addr;
>>>  dst_in = (struct sockaddr *) &req->dst_addr;
>>>  if (src_addr) memcpy(src_in, src_addr, rdma_addr_size(src_addr));
>>>  else src_in->sa_family = dst_addr->sa_family;
>>>  memcpy(dst_in, dst_addr, rdma_addr_size(dst_addr));
>>>  req->addr = addr;
>>>  req->callback = callback;
>>>  req->context = context;
>>>  req->resolve_by_gid_attr = resolve_by_gid_attr;
>>>  INIT_DELAYED_WORK(&req->work, process_one_req);
>>>  req->seq = (u32)atomic_inc_return(&ib_nl_addr_request_seq);
>>>  req->status = addr_resolve(src_in, dst_in, addr, true, req->resolve_by_gid_attr, req->seq);
>>>> if (resolve_by_gid_attr) set_addr_netns_by_gid_rcu(addr);
>>>> if (src_in->sa_family == AF_INET) {
>>>>  addr4_resolve(src_in, dst_in, addr, &rt);
>>>>> struct sockaddr_in *src_in = (struct sockaddr_in *)src_sock;
>>>>> const struct sockaddr_in *dst_in = (const struct sockaddr_in *)dst_sock;
>>>>> __be32 src_ip = src_in->sin_addr.s_addr;
>>>>> __be32 dst_ip = dst_in->sin_addr.s_addr;
>>>>> memset(&fl4, 0, sizeof(fl4));
>>>>> fl4.daddr = dst_ip;
>>>>> fl4.saddr = src_ip;
>>>>> fl4.flowi4_oif = addr->bound_dev_if;
>>>>> rt = ip_route_output_key(addr->net, &fl4);
>>>>> ret = PTR_ERR_OR_ZERO(rt);
>>>>> src_in->sin_addr.s_addr = fl4.saddr;
>>>>> addr->hoplimit = ip4_dst_hoplimit(&rt->dst);
>>>>> *prt = rt;
>>>>  dst = &rt->dst;
>>>> } else {
>>>>  addr6_resolve(src_in, dst_in, addr, &dst);
>>>> }
>>>> ret = rdma_set_src_addr_rcu(addr, &ndev_flags, dst_in, dst);
>>>>> struct net_device *ndev = READ_ONCE(dst->dev);
>>>>> *ndev_flags = ndev->flags;
>>>>> if (ndev->flags & IFF_LOOPBACK) ndev = rdma_find_ndev_for_src_ip_rcu(dev_net(ndev), dst_in);
>>>>>> switch (src_in->sa_family) {
>>>>>> case AF_INET: dev = __ip_dev_find(net, ((const struct sockaddr_in *)src_in)->sin_addr.s_addr, false);
>>>>>> case AF_INET6: for_each_netdev_rcu(net, dev) ipv6_chk_addr(net, &((const struct sockaddr_in6 *)src_in)->sin6_addr, dev, 1))
>>>>> return copy_src_l2_addr(dev_addr, dst_in, dst, ndev);
>>>>>> if (dst->dev->flags & IFF_LOOPBACK) rdma_translate_ip(dst_in, dev_addr);
>>>>>>> if (dev_addr->bound_dev_if) {
>>>>>>>  dev = dev_get_by_index(dev_addr->net, dev_addr->bound_dev_if);
>>>>>>>>  dev = dev_get_by_index_rcu(net, ifindex);
>>>>>>>>  EXPORT_SYMBOL(dev_get_by_index);
>>>>>>>  rdma_copy_src_l2_addr(dev_addr, dev);
>>>>>>>> dev_addr->dev_type = dev->type;
>>>>>>>> memcpy(dev_addr->src_dev_addr, dev->dev_addr, MAX_ADDR_LEN);
>>>>>>>> memcpy(dev_addr->broadcast, dev->broadcast, MAX_ADDR_LEN);
>>>>>>>> dev_addr->bound_dev_if = dev->ifindex;
>>>>>>>> EXPORT_SYMBOL(rdma_copy_src_l2_addr);
>>>>>>>  dev_put(dev);
>>>>>>> }
>>>>>>> dev = rdma_find_ndev_for_src_ip_rcu(dev_addr->net, addr);
>>>>>> else rdma_copy_src_l2_addr(dev_addr, dst->dev);
>>>>>> if (has_gateway(dst, dst_in->sa_family) && ndev->type != ARPHRD_INFINIBAND)
>>>>>>  dev_addr->network = dst_in->sa_family == AF_INET ? RDMA_NETWORK_IPV4:RDMA_NETWORK_IPV6;
>>>>>> else dev_addr->network = RDMA_NETWORK_IB;
>>>> if (!ret && resolve_neigh) addr_resolve_neigh(dst, dst_in, addr, ndev_flags, seq);
>>>>> if (ndev_flags & IFF_LOOPBACK) memcpy(addr->dst_dev_addr, addr->src_dev_addr, MAX_ADDR_LEN);
>>>>> else {if (!(ndev_flags & IFF_NOARP)) ret = fetch_ha(dst, addr, dst_in, seq);}
>>>>>> const struct sockaddr_in *dst_in4 = (const struct sockaddr_in *)dst_in;
>>>>>> const struct sockaddr_in6 *dst_in6 = (const struct sockaddr_in6 *)dst_in;
>>>>>> const void *daddr = (dst_in->sa_family == AF_INET) ? (const void *)&dst_in4->sin_addr.s_addr : (const void *)&dst_in6->sin6_addr;
>>>>>> sa_family_t family = dst_in->sa_family;
>>>>>> if (has_gateway(dst, family) && dev_addr->network == RDMA_NETWORK_IB) ib_nl_fetch_ha(dev_addr, daddr, seq, family);
>>>>>>> ib_nl_ip_send_msg(dev_addr, daddr, seq, family);
>>>>>> else dst_fetch_ha(dst, dev_addr, daddr);
>>>>>>>> n = dst_neigh_lookup(dst, daddr);
>>>>>>>> if (!(n->nud_state & NUD_VALID)) neigh_event_send(n, NULL);
>>>>>>>> else neigh_ha_snapshot(dev_addr->dst_dev_addr, n, dst->dev);
>>>>>>>> neigh_release(n);
>>>> if (src_in->sa_family == AF_INET) ip_rt_put(rt);
>>>> else dst_release(dst);
>>>  switch (req->status)
>>>  case 0: req->timeout = jiffies; queue_req(req);
>>>  case -ENODATA: eq->timeout = msecs_to_jiffies(timeout_ms) + jiffies; queue_req(req);
>> }
>> EXPORT_SYMBOL(rdma_resolve_addr);
> ucma_put_ctx(ctx);

static ssize_t ucma_connect(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> ucma_copy_conn_param(ctx->cm_id, &conn_param, &cmd.conn_param);
>> dst->private_data = src->private_data;
>> dst->private_data_len = src->private_data_len;
>> dst->responder_resources =src->responder_resources;
>> dst->initiator_depth = src->initiator_depth;
>> dst->flow_control = src->flow_control;
>> dst->retry_count = src->retry_count;
>> dst->rnr_retry_count = src->rnr_retry_count;
>> dst->srq = src->srq;
>> dst->qp_num = src->qp_num;
>> dst->qkey = (id->route.addr.src_addr.ss_family == AF_IB) ? src->qkey : 0;
> rdma_connect(ctx->cm_id, &conn_param);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (!cma_comp_exch(id_priv, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_CONNECT)) return -EINVAL;
>> if (!id->qp) id_priv->qp_num = conn_param->qp_num; id_priv->srq = conn_param->srq;
>> if (rdma_cap_ib_cm(id->device, id->port_num)) {
>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_IB_CM;
>>  if (id->qp_type == IB_QPT_UD) cma_resolve_ib_udp(id_priv, conn_param);
>>> memset(&req, 0, sizeof req);
>>> offset = cma_user_data_offset(id_priv);
>>> req.private_data_len = offset + conn_param->private_data_len;
>>> if (req.private_data_len < conn_param->private_data_len)
>>> if (req.private_data_len) private_data = kzalloc(req.private_data_len, GFP_ATOMIC);
>>> else private_data = NULL;
>>> if (conn_param->private_data && conn_param->private_data_len)
>>>  memcpy(private_data + offset, conn_param->private_data, conn_param->private_data_len);
>>> if (private_data) {
>>>  ret = cma_format_hdr(private_data, id_priv);
>>>> cma_hdr = hdr;
>>>> cma_hdr->cma_version = CMA_VERSION;
>>>> if (cma_family(id_priv) == AF_INET) {
>>>>  struct sockaddr_in *src4, *dst4;
>>>>  src4 = (struct sockaddr_in *) cma_src_addr(id_priv);
>>>>  dst4 = (struct sockaddr_in *) cma_dst_addr(id_priv);
>>>>  cma_set_ip_ver(cma_hdr, 4);
>>>>  cma_hdr->src_addr.ip4.addr = src4->sin_addr.s_addr;
>>>>  cma_hdr->dst_addr.ip4.addr = dst4->sin_addr.s_addr;
>>>>  cma_hdr->port = src4->sin_port;
>>>> } else if (cma_family(id_priv) == AF_INET6) {
>>>>  struct sockaddr_in6 *src6, *dst6;
>>>>  src6 = (struct sockaddr_in6 *) cma_src_addr(id_priv);
>>>>  dst6 = (struct sockaddr_in6 *) cma_dst_addr(id_priv);
>>>>  cma_set_ip_ver(cma_hdr, 6);
>>>>  cma_hdr->src_addr.ip6 = src6->sin6_addr;
>>>>  cma_hdr->dst_addr.ip6 = dst6->sin6_addr;
>>>>  cma_hdr->port = src6->sin6_port;
>>>> }
>>>  req.private_data = private_data;
>>> }
>>> id = ib_create_cm_id(id_priv->id.device, cma_sidr_rep_handler, id_priv);
>>>> cm_id_priv = kzalloc(sizeof *cm_id_priv, GFP_KERNEL);
>>>> cm_id_priv->id.state = IB_CM_IDLE;
>>>> cm_id_priv->id.device = device;
>>>> cm_id_priv->id.cm_handler = cm_handler;
>>>> cm_id_priv->id.context = context;
>>>> cm_id_priv->id.remote_cm_qpn = 1;
>>>> init_completion(&cm_id_priv->comp);
>>>> INIT_LIST_HEAD(&cm_id_priv->work_list);
>>>> INIT_LIST_HEAD(&cm_id_priv->prim_list);
>>>> INIT_LIST_HEAD(&cm_id_priv->altr_list);
>>>> atomic_set(&cm_id_priv->work_count, -1);
>>>> atomic_set(&cm_id_priv->refcount, 1);
>>>> xa_alloc_cyclic_irq(&cm.local_id_table, &id, NULL, xa_limit_32b, &cm.local_id_next, GFP_KERNEL);
>>>> cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
>>>> xa_store_irq(&cm.local_id_table, cm_local_id(cm_id_priv->id.local_id), cm_id_priv, GFP_KERNEL);
>>>> return &cm_id_priv->id;
>>>> EXPORT_SYMBOL(ib_create_cm_id);
>>> id_priv->cm_id.ib = id;
>>> req.path = id_priv->id.route.path_rec;
>>> req.sgid_attr = id_priv->id.route.addr.dev_addr.sgid_attr;
>>> req.service_id = rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv));
>>> req.timeout_ms = 1 << (CMA_CM_RESPONSE_TIMEOUT - 8);
>>> req.max_cm_retries = CMA_MAX_CM_RETRIES;
>>> ib_send_cm_sidr_req(id_priv->cm_id.ib, &req);
>>>> cm_id_priv = container_of(cm_id, struct cm_id_private, id);
>>>> ret = cm_init_av_by_path(param->path, param->sgid_attr, &cm_id_priv->av, cm_id_priv);
>>>> cm_id->service_id = param->service_id;
>>>> cm_id->service_mask = ~cpu_to_be64(0);
>>>> cm_id_priv->timeout_ms = param->timeout_ms;
>>>> cm_id_priv->max_cm_retries = param->max_cm_retries;
>>>> ret = cm_alloc_msg(cm_id_priv, &msg);
>>>> cm_format_sidr_req((struct cm_sidr_req_msg *) msg->mad, cm_id_priv, param);
>>>> msg->timeout_ms = cm_id_priv->timeout_ms;
>>>> msg->context[1] = (void *) (unsigned long) IB_CM_SIDR_REQ_SENT;
>>>> if (cm_id->state == IB_CM_IDLE) ib_post_send_mad(msg, NULL);
>>>>> for (; send_buf; send_buf = next_send_buf) {
>>>>> 	mad_send_wr = container_of(send_buf, struct ib_mad_send_wr_private, send_buf);
>>>>> 	mad_agent_priv = mad_send_wr->mad_agent_priv;
>>>>> 	ret = ib_mad_enforce_security(mad_agent_priv, mad_send_wr->send_wr.pkey_index);
>>>>> 	next_send_buf = send_buf->next;
>>>>> 	mad_send_wr->send_wr.ah = send_buf->ah;
>>>>> 	if (((struct ib_mad_hdr *) send_buf->mad)->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {
>>>>> 	 ret = handle_outgoing_dr_smp(mad_agent_priv, mad_send_wr);
>>>>> 	}
>>>>> 	mad_send_wr->tid = ((struct ib_mad_hdr *) send_buf->mad)->tid;
>>>>> 	mad_send_wr->timeout = msecs_to_jiffies(send_buf->timeout_ms);
>>>>> 	mad_send_wr->max_retries = send_buf->retries;
>>>>> 	mad_send_wr->retries_left = send_buf->retries;
>>>>> 	send_buf->retries = 0;
>>>>> 	mad_send_wr->refcount = 1 + (mad_send_wr->timeout > 0);
>>>>> 	mad_send_wr->status = IB_WC_SUCCESS;
>>>>> 	atomic_inc(&mad_agent_priv->refcount);
>>>>> 	list_add_tail(&mad_send_wr->agent_list, &mad_agent_priv->send_list);
>>>>> 	if (ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)) {
>>>>> 	 ret = ib_send_rmpp_mad(mad_send_wr);
>>>>> 	 if (ret >= 0 && ret != IB_RMPP_RESULT_CONSUMED)
>>>>> 	  ret = ib_send_mad(mad_send_wr);
>>>>> 	} else ib_send_mad(mad_send_wr);
>>>>> }
>>>> cm_id->state = IB_CM_SIDR_REQ_SENT;
>>>> cm_id_priv->msg = msg;
>>  else cma_connect_ib(id_priv, conn_param);
>> } else if (rdma_cap_iw_cm(id->device, id->port_num)) cma_connect_iw(id_priv, conn_param);
> ucma_put_ctx(ctx);

static ssize_t ucma_get_event(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> uevent = list_entry(file->event_list.next, struct ucma_event, list);
> if (uevent->resp.event == RDMA_CM_EVENT_CONNECT_REQUEST) {
> ctx = ucma_alloc_ctx(file);
>> ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
>> INIT_WORK(&ctx->close_work, ucma_close_id);
>> init_completion(&ctx->comp);
>> INIT_LIST_HEAD(&ctx->mc_list);
>> ctx->file = file;
>> xa_alloc(&ctx_table, &ctx->id, ctx, xa_limit_32b, GFP_KERNEL)
>> list_add_tail(&ctx->list, &file->ctx_list);
> uevent->ctx->backlog++;
> ctx->cm_id = uevent->cm_id;
> ctx->cm_id->context = ctx;
> uevent->resp.id = ctx->id;
> }
> copy_to_user(u64_to_user_ptr(cmd.response), &uevent->resp, min_t(size_t, out_len, sizeof(uevent->resp)))
> list_del(&uevent->list);
> uevent->ctx->events_reported++;
> if (uevent->mc) uevent->mc->events_reported++;
> kfree(uevent);

static ssize_t ucma_resolve_route(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> ret = rdma_resolve_route(ctx->cm_id, cmd.timeout_ms);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (!cma_comp_exch(id_priv, RDMA_CM_ADDR_RESOLVED, RDMA_CM_ROUTE_QUERY)) return -EINVAL;
>> if (rdma_cap_ib_sa(id->device, id->port_num)) cma_resolve_ib_route(id_priv, timeout_ms);
>>> struct rdma_route *route = &id_priv->id.route;
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work->old_state = RDMA_CM_ROUTE_QUERY;
>>>> work->new_state = RDMA_CM_ROUTE_RESOLVED;
>>>> work->event.event = RDMA_CM_EVENT_ROUTE_RESOLVED;
>>> route->path_rec = kmalloc(sizeof *route->path_rec, GFP_KERNEL);
>>> cma_query_ib_route(id_priv, timeout_ms, work);
>>>> struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
>>>> memset(&path_rec, 0, sizeof path_rec);
>>>> if (rdma_cap_opa_ah(id_priv->id.device, id_priv->id.port_num)) path_rec.rec_type = SA_PATH_REC_TYPE_OPA;
>>>> else path_rec.rec_type = SA_PATH_REC_TYPE_IB;
>>>> rdma_addr_get_sgid(dev_addr, &path_rec.sgid);
>>>> rdma_addr_get_dgid(dev_addr, &path_rec.dgid);
>>>> path_rec.pkey = cpu_to_be16(ib_addr_get_pkey(dev_addr));
>>>> path_rec.numb_path = 1;
>>>> path_rec.reversible = 1;
>>>> path_rec.service_id = rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv));
>>>>> if (addr->sa_family == AF_IB) return ((struct sockaddr_ib *) addr)->sib_sid;
>>>>> return cpu_to_be64(((u64)id->ps << 16) + be16_to_cpu(cma_port(addr)));
>>>> comp_mask = IB_SA_PATH_REC_DGID | IB_SA_PATH_REC_SGID |IB_SA_PATH_REC_PKEY | IB_SA_PATH_REC_NUMB_PATH |
>>>> 	         IB_SA_PATH_REC_REVERSIBLE | IB_SA_PATH_REC_SERVICE_ID;
>>>> switch (cma_family(id_priv)) {
>>>> case AF_INET:
>>>>  path_rec.qos_class = cpu_to_be16((u16) id_priv->tos);
>>>>  comp_mask |= IB_SA_PATH_REC_QOS_CLASS;
>>>> case AF_INET6:
>>>>  sin6 = (struct sockaddr_in6 *) cma_src_addr(id_priv);
>>>>> (struct sockaddr *) &id_priv->id.route.addr.src_addr;
>>>>  path_rec.traffic_class = (u8) (be32_to_cpu(sin6->sin6_flowinfo) >> 20);
>>>>  comp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;
>>>> case AF_IB:
>>>>  sib = (struct sockaddr_ib *) cma_src_addr(id_priv);
>>>>> (struct sockaddr *) &id_priv->id.route.addr.src_addr;
>>>>  path_rec.traffic_class = (u8) (be32_to_cpu(sib->sib_flowinfo) >> 20);
>>>>  comp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;
>>>> }
>>>> id_priv->query_id = ib_sa_path_rec_get(&sa_client, id_priv->id.device,id_priv->id.port_num, &path_rec, comp_mask, timeout_ms, GFP_KERNEL, cma_query_handler, work, &id_priv->query);
>>>>> struct ib_sa_device *sa_dev = ib_get_client_data(device, &sa_client);
>>>>> port  = &sa_dev->port[port_num - sa_dev->start_port];
>>>>> agent = port->agent;
>>>>> query = kzalloc(sizeof(*query), gfp_mask);
>>>>> query->sa_query.port     = port;
>>>>> if (rec->rec_type == SA_PATH_REC_TYPE_OPA) {
>>>>> 	status = opa_pr_query_possible(client, device, port_num, rec);
>>>>> 	if (status == PR_NOT_SUPPORTED)	ret = -EINVAL;
>>>>> 	else if (status == PR_OPA_SUPPORTED) query->sa_query.flags |= IB_SA_QUERY_OPA;
>>>>> 	else query->conv_pr = kmalloc(sizeof(*query->conv_pr), gfp_mask);
>>>>> }
>>>>> ret = alloc_mad(&query->sa_query, gfp_mask);
>>>>> ib_sa_client_get(client);
>>>>> query->sa_query.client = client;
>>>>> query->callback        = callback;
>>>>> query->context         = context;
>>>>> mad = query->sa_query.mad_buf->mad;
>>>>> init_mad(&query->sa_query, agent);
>>>>> query->sa_query.callback = callback ? ib_sa_path_rec_callback : NULL;
>>>>> query->sa_query.release  = ib_sa_path_rec_release;
>>>>> mad->mad_hdr.method	 = IB_MGMT_METHOD_GET;
>>>>> mad->mad_hdr.attr_id	 = cpu_to_be16(IB_SA_ATTR_PATH_REC);
>>>>> mad->sa_hdr.comp_mask	 = comp_mask;
>>>>> if (query->sa_query.flags & IB_SA_QUERY_OPA) {
>>>>> 	ib_pack(opa_path_rec_table, ARRAY_SIZE(opa_path_rec_table), rec, mad->data);
>>>>> } else if (query->conv_pr) {
>>>>> 	sa_convert_path_opa_to_ib(query->conv_pr, rec);
>>>>> 	ib_pack(path_rec_table, ARRAY_SIZE(path_rec_table), query->conv_pr, mad->data);
>>>>> } else {
>>>>> 	ib_pack(path_rec_table, ARRAY_SIZE(path_rec_table), rec, mad->data);
>>>>> }
>>>>> *sa_query = &query->sa_query;
>>>>> query->sa_query.flags |= IB_SA_ENABLE_LOCAL_SERVICE;
>>>>> query->sa_query.mad_buf->context[1] = (query->conv_pr) ? query->conv_pr : rec;
>>>>> ret = send_mad(&query->sa_query, timeout_ms, gfp_mask);
>>>>>> xa_lock_irqsave(&queries, flags);
>>>>>> ret = __xa_alloc(&queries, &id, query, xa_limit_32b, gfp_mask);
>>>>>> xa_unlock_irqrestore(&queries, flags);
>>>>>> query->mad_buf->timeout_ms  = timeout_ms;
>>>>>> query->mad_buf->context[0] = query;
>>>>>> query->id = id;
>>>>>> if ((query->flags & IB_SA_ENABLE_LOCAL_SERVICE) && (!(query->flags & IB_SA_QUERY_OPA))) {
>>>>>> 	if (rdma_nl_chk_listeners(RDMA_NL_GROUP_LS)) {
>>>>>> 	 if (!ib_nl_make_request(query, gfp_mask)) return id;
>>>>>> 	}
>>>>>> 	ib_sa_disable_local_svc(query);
>>>>>> }
>>>>>> ret = ib_post_send_mad(query->mad_buf, NULL);
>>>>>> if (ret) {
>>>>>> 	xa_lock_irqsave(&queries, flags);
>>>>>> 	__xa_erase(&queries, id);
>>>>>> 	xa_unlock_irqrestore(&queries, flags);
>>>>>> }
>>>>>> return ret ? ret : id;
>>>> return (id_priv->query_id < 0) ? id_priv->query_id : 0;
>> else if (rdma_protocol_roce(id->device, id->port_num)) cma_resolve_iboe_route(id_priv);
>>> struct rdma_route *route = &id_priv->id.route;
>>> struct rdma_addr *addr = &route->addr;
>>> u8 default_roce_tos = id_priv->cma_dev->default_roce_tos[id_priv->id.port_num - rdma_start_port(id_priv->cma_dev->device)];
>>> u8 tos = id_priv->tos_set ? id_priv->tos : default_roce_tos;
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> route->path_rec = kzalloc(sizeof *route->path_rec, GFP_KERNEL);
>>> route->num_paths = 1;
>>> ndev = cma_iboe_set_path_rec_l2_fields(id_priv);
>>> rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr, &route->path_rec->sgid);
>>> rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.dst_addr, &route->path_rec->dgid);
>>> if (((struct sockaddr *)&id_priv->id.route.addr.dst_addr)->sa_family != AF_IB)
>>>  route->path_rec->hop_limit = addr->dev_addr.hoplimit;
>>> else route->path_rec->hop_limit = 1;
>>> route->path_rec->reversible = 1;
>>> route->path_rec->pkey = cpu_to_be16(0xffff);
>>> route->path_rec->mtu_selector = IB_SA_EQ;
>>> route->path_rec->sl = iboe_tos_to_sl(ndev, tos);
>>> route->path_rec->traffic_class = tos;
>>> route->path_rec->mtu = iboe_get_mtu(ndev->mtu);
>>> route->path_rec->rate_selector = IB_SA_EQ;
>>> route->path_rec->rate = iboe_get_rate(ndev);
>>> dev_put(ndev);
>>>> this_cpu_dec(*dev->pcpu_refcnt);
>>> route->path_rec->packet_life_time_selector = IB_SA_EQ;
>>> route->path_rec->packet_life_time = CMA_IBOE_PACKET_LIFETIME;
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work->old_state = RDMA_CM_ROUTE_QUERY;
>>>> work->new_state = RDMA_CM_ROUTE_RESOLVED;
>>>> work->event.event = RDMA_CM_EVENT_ROUTE_RESOLVED;
>>> queue_work(cma_wq, &work->work);
>> else if (rdma_protocol_iwarp(id->device, id->port_num)) cma_resolve_iw_route(id_priv);
>>> work = kzalloc(sizeof *work, GFP_KERNEL);
>>> cma_init_resolve_route_work(work, id_priv);
>>>> work->id = id_priv;
>>>> INIT_WORK(&work->work, cma_work_handler);
>>>> work->old_state = RDMA_CM_ROUTE_QUERY;
>>>> work->new_state = RDMA_CM_ROUTE_RESOLVED;
>>>> work->event.event = RDMA_CM_EVENT_ROUTE_RESOLVED;
>>> queue_work(cma_wq, &work->work);
>> else ret = -ENOSYS;
> ucma_put_ctx(ctx);
> return ret;

static ssize_t ucma_init_qp_attr(struct ucma_file *file, const char __user *inbuf, int in_len, int out_len)
> copy_from_user(&cmd, inbuf, sizeof(cmd))
> ctx = ucma_get_ctx_dev(file, cmd.id);
>> struct ucma_context *ctx = ucma_get_ctx(file, id);
>>> ctx = _ucma_find_context(id, file);
>>>> ctx = xa_load(&ctx_table, id);
>>>>> XA_STATE(xas, xa, index);
>>>>> do { entry = xas_load(&xas);
>>>>>> void *entry = xas_start(xas);
>>>>>> while (xa_is_node(entry)) {
>>>>>> 	struct xa_node *node = xa_to_node(entry);
>>>>>> 	if (xas->xa_shift > node->shift) break;
>>>>>> 	entry = xas_descend(xas, node);
>>>>>> 	if (node->shift == 0) break;
>>>>>> }
>>>>> } while (xas_retry(&xas, entry));
> resp.qp_attr_mask = 0;
> memset(&qp_attr, 0, sizeof qp_attr);
> qp_attr.qp_state = cmd.qp_state;
> rdma_init_qp_attr(ctx->cm_id, &qp_attr, &resp.qp_attr_mask);
>> id_priv = container_of(id, struct rdma_id_private, id);
>> if (rdma_cap_ib_cm(id->device, id->port_num)) {
>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_IB_CM;
>>  if (!id_priv->cm_id.ib || (id_priv->id.qp_type == IB_QPT_UD)) cma_ib_init_qp_attr(id_priv, qp_attr, qp_attr_mask);
>>>  struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
>>>  if (rdma_cap_eth_ah(id_priv->id.device, id_priv->id.port_num)) pkey = 0xffff;
>>>>  device->port_data[port_num].immutable.core_cap_flags & RDMA_CORE_CAP_ETH_AH;
>>>  else pkey = ib_addr_get_pkey(dev_addr);
>>>>  ((u16)dev_addr->broadcast[8] << 8) | (u16)dev_addr->broadcast[9];
>>>  ret = ib_find_cached_pkey(id_priv->id.device, id_priv->id.port_num, pkey, &qp_attr->pkey_index);
>>>>  if (!rdma_is_port_valid(device, port_num))
>>>>> (port >= rdma_start_port(device) && port <= rdma_end_port(device));
>>>>  cache = device->port_data[port_num].cache.pkey;
>>>>  *index = -1;
>>>>  for (i = 0; i < cache->table_len; ++i)
>>>>  	if ((cache->table[i] & 0x7fff) == (pkey & 0x7fff)) {
>>>>  	 if (cache->table[i] & 0x8000) *index = i; ret = 0;
>>>>  	 else partial_ix = i;
>>>>  	}
>>>>  if (ret && partial_ix >= 0) *index = partial_ix; ret = 0;
>>>  qp_attr->port_num = id_priv->id.port_num;
>>>  *qp_attr_mask = IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT;
>>>  if (id_priv->id.qp_type == IB_QPT_UD) {
>>>   cma_set_qkey(id_priv, 0);
>>>>   if (qkey) id_priv->qkey = qkey;
>>>>   switch (id_priv->id.ps)
>>>>   case RDMA_PS_UDP:
>>>>   case RDMA_PS_IB: id_priv->qkey = RDMA_UDP_QKEY;
>>>>   case RDMA_PS_IPOIB:
>>>>    ib_addr_get_mgid(&id_priv->id.route.addr.dev_addr, &rec.mgid);
>>>>>    memcpy(gid, dev_addr->broadcast + 4, sizeof *gid);
>>>>    ib_sa_get_mcmember_rec(id_priv->id.device, id_priv->id.port_num, &rec.mgid, &rec);
>>>>>    dev = ib_get_client_data(device, &mcast_client);
>>>>>     xa_load(&device->client_data, client->client_id);
>>>>>    port = &dev->port[port_num - dev->start_port];
>>>>>    group = mcast_find(port, mgid);
>>>>>    if (group)*rec = group->rec;
>>>   qp_attr->qkey = id_priv->qkey;
>>>   *qp_attr_mask |= IB_QP_QKEY;
>>>  } else {
>>>   qp_attr->qp_access_flags = 0;
>>>   *qp_attr_mask |= IB_QP_ACCESS_FLAGS;
>>>  }
>>  else ret = ib_cm_init_qp_attr(id_priv->cm_id.ib, qp_attr, qp_attr_mask);
>>>  cm_id_priv = container_of(cm_id, struct cm_id_private, id);
>>>  switch (qp_attr->qp_state)
>>>  case IB_QPS_INIT: ret = cm_init_qp_init_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->id.state)
>>>>  case IB_CM_REQ_SENT:
>>>>  case IB_CM_MRA_REQ_RCVD:
>>>>  case IB_CM_REQ_RCVD:
>>>>  case IB_CM_MRA_REQ_SENT:
>>>>  case IB_CM_REP_RCVD:
>>>>  case IB_CM_MRA_REP_SENT:
>>>>  case IB_CM_REP_SENT:
>>>>  case IB_CM_MRA_REP_RCVD:
>>>>  case IB_CM_ESTABLISHED:
>>>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PKEY_INDEX | IB_QP_PORT;
>>>>   qp_attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE;
>>>>   if (cm_id_priv->responder_resources) qp_attr->qp_access_flags |= IB_ACCESS_REMOTE_READ | IB_ACCESS_REMOTE_ATOMIC;
>>>>   qp_attr->pkey_index = cm_id_priv->av.pkey_index;
>>>>   qp_attr->port_num = cm_id_priv->av.port->port_num;
>>>  case IB_QPS_RTR: ret = cm_init_qp_rtr_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  case IB_CM_REQ_RCVD:
>>>>  case IB_CM_MRA_REQ_SENT:
>>>>  case IB_CM_REP_RCVD:
>>>>  case IB_CM_MRA_REP_SENT:
>>>>  case IB_CM_REP_SENT:
>>>>  case IB_CM_MRA_REP_RCVD:
>>>>  case IB_CM_ESTABLISHED:
>>>>  	*qp_attr_mask = IB_QP_STATE | IB_QP_AV | IB_QP_PATH_MTU | IB_QP_DEST_QPN | IB_QP_RQ_PSN;
>>>>  	qp_attr->ah_attr = cm_id_priv->av.ah_attr;
>>>>  	qp_attr->path_mtu = cm_id_priv->path_mtu;
>>>>  	qp_attr->dest_qp_num = be32_to_cpu(cm_id_priv->remote_qpn);
>>>>  	qp_attr->rq_psn = be32_to_cpu(cm_id_priv->rq_psn);
>>>>  	if (cm_id_priv->qp_type == IB_QPT_RC ||
>>>>  	    cm_id_priv->qp_type == IB_QPT_XRC_TGT) {
>>>>  		*qp_attr_mask |= IB_QP_MAX_DEST_RD_ATOMIC | IB_QP_MIN_RNR_TIMER;
>>>>  		qp_attr->max_dest_rd_atomic = cm_id_priv->responder_resources;
>>>>  		qp_attr->min_rnr_timer = 0;
>>>>  	}
>>>>  	if (rdma_ah_get_dlid(&cm_id_priv->alt_av.ah_attr)) {
>>>>  		*qp_attr_mask |= IB_QP_ALT_PATH;
>>>>  		qp_attr->alt_port_num = cm_id_priv->alt_av.port->port_num;
>>>>  		qp_attr->alt_pkey_index = cm_id_priv->alt_av.pkey_index;
>>>>  		qp_attr->alt_timeout = cm_id_priv->alt_av.timeout;
>>>>  		qp_attr->alt_ah_attr = cm_id_priv->alt_av.ah_attr;
>>>>  	}
>>>  case IB_QPS_RTS: ret = cm_init_qp_rts_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->id.state) {
>>>>  case IB_CM_REQ_RCVD:
>>>>  case IB_CM_MRA_REQ_SENT:
>>>>  case IB_CM_REP_RCVD:
>>>>  case IB_CM_MRA_REP_SENT:
>>>>  case IB_CM_REP_SENT:
>>>>  case IB_CM_MRA_REP_RCVD:
>>>>  case IB_CM_ESTABLISHED:
>>>>   if (cm_id_priv->id.lap_state == IB_CM_LAP_UNINIT) {
>>>>    *qp_attr_mask = IB_QP_STATE | IB_QP_SQ_PSN;
>>>>    qp_attr->sq_psn = be32_to_cpu(cm_id_priv->sq_psn);
>>>>    switch (cm_id_priv->qp_type) {
>>>>    case IB_QPT_RC:
>>>>    case IB_QPT_XRC_INI:
>>>>    	*qp_attr_mask |= IB_QP_RETRY_CNT | IB_QP_RNR_RETRY |
>>>>    			 IB_QP_MAX_QP_RD_ATOMIC;
>>>>    	qp_attr->retry_cnt = cm_id_priv->retry_count;
>>>>    	qp_attr->rnr_retry = cm_id_priv->rnr_retry_count;
>>>>    	qp_attr->max_rd_atomic = cm_id_priv->initiator_depth;
>>>>    case IB_QPT_XRC_TGT:
>>>>    	*qp_attr_mask |= IB_QP_TIMEOUT;
>>>>    	qp_attr->timeout = cm_id_priv->av.timeout;
>>>>    }
>>>>    if (rdma_ah_get_dlid(&cm_id_priv->alt_av.ah_attr)) {
>>>>    	*qp_attr_mask |= IB_QP_PATH_MIG_STATE;
>>>>    	qp_attr->path_mig_state = IB_MIG_REARM;
>>>>    }
>>>>   } else {
>>>>    *qp_attr_mask = IB_QP_ALT_PATH | IB_QP_PATH_MIG_STATE;
>>>>    qp_attr->alt_port_num = cm_id_priv->alt_av.port->port_num;
>>>>    qp_attr->alt_pkey_index = cm_id_priv->alt_av.pkey_index;
>>>>    qp_attr->alt_timeout = cm_id_priv->alt_av.timeout;
>>>>    qp_attr->alt_ah_attr = cm_id_priv->alt_av.ah_attr;
>>>>    qp_attr->path_mig_state = IB_MIG_REARM;
>>>>   }
>>  if (qp_attr->qp_state == IB_QPS_RTR) qp_attr->rq_psn = id_priv->seq_num;
>> } else if (rdma_cap_iw_cm(id->device, id->port_num)) {
>>  if (!id_priv->cm_id.iw) {
>>   qp_attr->qp_access_flags = 0;
>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS;
>>  }
>>  else ret = iw_cm_init_qp_attr(id_priv->cm_id.iw, qp_attr, qp_attr_mask);
>>>  cm_id_priv = container_of(cm_id, struct iwcm_id_private, id);
>>>  switch (qp_attr->qp_state) {
>>>  case IB_QPS_INIT:
>>>  case IB_QPS_RTR: iwcm_init_qp_init_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->state) {
>>>>  case IW_CM_STATE_IDLE:
>>>>  case IW_CM_STATE_CONN_SENT:
>>>>  case IW_CM_STATE_CONN_RECV:
>>>>  case IW_CM_STATE_ESTABLISHED:
>>>>   *qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS;
>>>>  	qp_attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE|IB_ACCESS_REMOTE_READ;
>>>  case IB_QPS_RTS: iwcm_init_qp_rts_attr(cm_id_priv, qp_attr, qp_attr_mask);
>>>>  switch (cm_id_priv->state) {
>>>>  case IW_CM_STATE_IDLE:
>>>>  case IW_CM_STATE_CONN_SENT:
>>>>  case IW_CM_STATE_CONN_RECV:
>>>>  case IW_CM_STATE_ESTABLISHED: *qp_attr_mask = 0;
>> qp_attr->port_num = id_priv->id.port_num;
>> *qp_attr_mask |= IB_QP_PORT;
>> }
>> if ((*qp_attr_mask & IB_QP_TIMEOUT) && id_priv->timeout_set)
>>   qp_attr->timeout = id_priv->timeout;
>> EXPORT_SYMBOL(rdma_init_qp_attr);
> ib_copy_qp_attr_to_user(ctx->cm_id->device, &resp, &qp_attr);
>> dst->qp_state	= src->qp_state;
>> dst->cur_qp_state	= src->cur_qp_state;
>> dst->path_mtu        = src->path_mtu;
>> dst->path_mig_state	= src->path_mig_state;
>> dst->qkey		= src->qkey;
>> dst->rq_psn		= src->rq_psn;
>> dst->sq_psn		= src->sq_psn;
>> dst->dest_qp_num	= src->dest_qp_num;
>> dst->qp_access_flags	= src->qp_access_flags;
>> dst->max_send_wr	= src->cap.max_send_wr;
>> dst->max_recv_wr	= src->cap.max_recv_wr;
>> dst->max_send_sge	= src->cap.max_send_sge;
>> dst->max_recv_sge	= src->cap.max_recv_sge;
>> dst->max_inline_data	= src->cap.max_inline_data;
>> ib_copy_ah_attr_to_user(device, &dst->ah_attr, &src->ah_attr);
>>> struct rdma_ah_attr *src = ah_attr;
>>> memset(&dst->grh.reserved, 0, sizeof(dst->grh.reserved));
>>> if ((ah_attr->type == RDMA_AH_ATTR_TYPE_OPA) &&
>>>  (rdma_ah_get_dlid(ah_attr) > be16_to_cpu(IB_LID_PERMISSIVE)) &&
>>>  (!rdma_ah_conv_opa_to_ib(device, &conv_ah, ah_attr)))
>>> src = &conv_ah;
>>> dst->dlid		   = rdma_ah_get_dlid(src);
>>> dst->sl	           = rdma_ah_get_sl(src);
>>> dst->src_path_bits	   = rdma_ah_get_path_bits(src);
>>> dst->static_rate	   = rdma_ah_get_static_rate(src);
>>> dst->is_global         = rdma_ah_get_ah_flags(src) & IB_AH_GRH ? 1 : 0;
>>> if (dst->is_global) {
>>>  const struct ib_global_route *grh = rdma_ah_read_grh(src);
>>>  memcpy(dst->grh.dgid, grh->dgid.raw, sizeof(grh->dgid));
>>>  dst->grh.flow_label        = grh->flow_label;
>>>  dst->grh.sgid_index        = grh->sgid_index;
>>>  dst->grh.hop_limit         = grh->hop_limit;
>>>  dst->grh.traffic_class     = grh->traffic_class;
>>> }
>>> dst->port_num		   = rdma_ah_get_port_num(src);
>>> dst->reserved 		   = 0;
>>> EXPORT_SYMBOL(ib_copy_ah_attr_to_user);

>> ib_copy_ah_attr_to_user(device, &dst->alt_ah_attr, &src->alt_ah_attr);
>> dst->pkey_index		= src->pkey_index;
>> dst->alt_pkey_index	= src->alt_pkey_index;
>> dst->en_sqd_async_notify = src->en_sqd_async_notify;
>> dst->sq_draining	= src->sq_draining;
>> dst->max_rd_atomic	= src->max_rd_atomic;
>> dst->max_dest_rd_atomic	= src->max_dest_rd_atomic;
>> dst->min_rnr_timer	= src->min_rnr_timer;
>> dst->port_num		= src->port_num;
>> dst->timeout		= src->timeout;
>> dst->retry_cnt		= src->retry_cnt;
>> dst->rnr_retry		= src->rnr_retry;
>> dst->alt_port_num	= src->alt_port_num;
>> dst->alt_timeout	= src->alt_timeout;
>> memset(dst->reserved, 0, sizeof(dst->reserved));
>> EXPORT_SYMBOL(ib_copy_qp_attr_to_user);
> copy_to_user(u64_to_user_ptr(cmd.response), &resp, sizeof(resp))

rpci_ep_module_init()
-> pci_epf_register_driver(&rpci_ep_driver);
-> rdma_link_register(&rpci_ep_link_ops);
->-> list_add(&ops->list, &link_ops)
-> irq_scheduled_flag = false;

rpci_ep_probe(epf)
-> epf->header = &rpci_ep_pci_header;
-> rdev = ib_alloc_device(rpci_ep_dev, ib_dev);
->-> device = alloc ib_device structure  
->-> device->groups[] = &ib_dev_attr_group
->-> rdma_init_coredev(&device->coredev, device, &init_net);
->->-> coredev->dev.class = &
->->-> coredev->dev.class = &ib_class;
->->-> coredev->dev.groups = dev->groups;
->->-> coredev->owner = dev;
->->-> INIT_LIST_HEAD(&coredev->port_list);
-> epf_set_drvdata(epf, rdev);
->-> epf->dev->driver_data = rdev
-> rdev->epf = epf
-> rdev->status = false
-> init rdev->{cmd_handler, irq_gen_handler)
-> rdev->wq = alloc_workqueue("rpci_ep_wq", WQ_MEM_RECLAIM | WQ_HIGHPRI, 0)
-> rpci_ep_init(rdev);
->-> rpci_ep_init_device_param(rdev);
->->-> init rdev->attr
->-> rpci_ep_init_ports(rdev);
->-> rpci_ep_init_pools(rdev);
-> dma_set_coherent_mask(dev, DMA_BIT_MASK(32))
->-> mask = (dma_addr_t)mask;
->-> dev->coherent_dma_mask = mask;

static struct rdma_link_ops rpci_ep_link_ops = {
	.type = "rpci-ep",
	.newlink = rpci_ep_newlink,
};
rpci_ep_newlink(ibdev_name,ndev)
-> rdev = inited_rdev;
-> rdev->ndev = ndev;
-> rpci_ep_set_mtu(rdev, 0x400);
-> pci_epc_start(rdev->epf->epc);
->-> epc->ops->start(epc);
->->-> pci->ops->start_link(pci)
-> rpci_ep_register_device(rdev, ibdev_name);
-> rdev->ready = true;

static const struct dw_pcie_ops dw_pcie_ep_ops = {
        .read_dbi = NULL,
        .write_dbi = NULL,
};

static const struct pci_epc_ops epc_ops = {
        .write_header           = dw_pcie_ep_write_header,
        .set_bar                = dw_pcie_ep_set_bar,
        .clear_bar              = dw_pcie_ep_clear_bar,
        .map_addr               = dw_pcie_ep_map_addr,
        .unmap_addr             = dw_pcie_ep_unmap_addr,
        .set_msi                = dw_pcie_ep_set_msi,
        .get_msi                = dw_pcie_ep_get_msi,
        .set_msix               = dw_pcie_ep_set_msix,
        .get_msix               = dw_pcie_ep_get_msix,
        .raise_irq              = dw_pcie_ep_raise_irq,
        .start                  = dw_pcie_ep_start,
        .stop                   = dw_pcie_ep_stop,
        .get_features           = dw_pcie_ep_get_features,
};

struct ib_qp *rpci_create_qp(struct ib_pd *ibpd, struct ib_qp_init_attr *init, struct ib_udata *udata)
-> rpci_ep_qp_chk_init(rpci, init);
-> qp = rpci_ep_alloc(&rpci->qp_pool);
	struct rpci_ep_pool_entry *elem;
	ib_device_try_get(&pool->rpci->ib_dev)
	elem = kzalloc(rpci_ep_type_info[pool->type].size, (pool->flags & RPCI_POOL_ATOMIC) ?  GFP_ATOMIC : GFP_KERNEL);
	elem->pool = pool;
-> rpci_ep_add_index(qp);
	elem->index = alloc_index(pool);
	insert_index(pool, elem);
-> rpci_ep_qp_from_init(rpci, qp, pd, init, uresp, ibpd, udata);
	qp->pd			= pd;
	qp->rcq			= rcq;
	qp->scq			= scq;
	qp->srq			= srq;
	rpci_ep_qp_init_misc(rpci, qp, init);
		qp->sq_sig_type		= init->sq_sig_type;
		qp->attr.path_mtu	= 1;
		qp->mtu			= ib_mtu_enum_to_int(qp->attr.path_mtu);
		qpn			= qp->pelem.index;
		port			= &rpci->port;
		qp->ibqp.qp_num		= 0;
		port->qp_smi_index	= qpn;
		qp->attr.port_num	= init->port_num;
	rpci_ep_qp_init_req(rpci, qp, init, udata, uresp);
		sock_create_kern(&init_net, AF_INET, SOCK_DGRAM, 0, &qp->sk);
		qp->sk->sk->sk_user_data = qp;
		qp->rpci = rpci;
		qp->src_port = RPCI_ROCE_V2_SPORT + (hash_32_generic(qp_num(qp), 14) & 0x3fff);
		qp->sq.max_wr		= init->cap.max_send_wr;
		qp->sq.max_sge		= init->cap.max_send_sge;
		qp->sq.max_inline	= init->cap.max_inline_data;
		wqe_size = max_t(int, sizeof(struct rxe_send_wqe) + qp->sq.max_sge * sizeof(struct ib_sge), sizeof(struct rxe_send_wqe) + qp->sq.max_inline);
		qp->sq.queue = rpci_ep_queue_init(rpci, &qp->sq.max_wr, wqe_size);
			q = kmalloc(sizeof(*q), GFP_KERNEL);
			q->rpci = rpci;
			q->elem_size = elem_size;
			elem_size = roundup_pow_of_two(elem_size);
			q->log2_elem_size = order_base_2(elem_size);
			num_slots = *num_elem + 1;
			num_slots = roundup_pow_of_two(num_slots);
			q->index_mask = num_slots - 1;
			buf_size = sizeof(struct rpci_queue_buf) + num_slots * elem_size;
			q->buf = vmalloc_user(buf_size);
			q->buf->log2_elem_size = q->log2_elem_size;
			q->buf->index_mask = q->index_mask;
			q->buf_size = buf_size;
			*num_elem = num_slots - 1;
		ep_do_mmap_info(rpci, uresp ? &uresp->sq_mi : NULL, udata, qp->sq.queue->buf, qp->sq.queue->buf_size, &qp->sq.queue->ip);
			if (outbuf) {
				ip = rpci_ep_create_mmap_info(rpci, buf_size, udata, buf);
					ip = kmalloc(sizeof(*ip), GFP_KERNEL);
					size = PAGE_ALIGN(size);
					ip->info.offset = rpci->mmap_offset;
					rpci->mmap_offset += ALIGN(size, SHMLBA);
					INIT_LIST_HEAD(&ip->pending_mmaps);
					ip->info.size = size;
					ip->context = container_of(udata, struct uverbs_attr_bundle, driver_udata)->context;
					ip->obj = obj;
				copy_to_user(outbuf, &ip->info, sizeof(ip->info));
				list_add(&ip->pending_mmaps, &rpci->pending_mmaps);
	}
	*ip_p = ip;
		qp->req.wqe_index	= producer_index(qp->sq.queue);
		qp->req.state		= QP_STATE_RESET;
		qp->req.opcode		= -1;
		qp->comp.opcode		= -1;
		rpci_ep_init_task(rpci, &qp->req.task, qp, rpci_ep_requester, "req");
			task->obj	= obj;
			task->arg	= arg;
			task->func	= func;
			snprintf(task->name, sizeof(task->name), "%s", name);
			task->destroyed	= false;
			tasklet_init(&task->tasklet, rpci_ep_do_task, (unsigned long)task);
			task->state = TASK_STATE_START;
		rpci_ep_init_task(rpci, &qp->comp.task, qp, rpci_ep_completer, "comp");
		qp->qp_timeout_jiffies = 0; /* Can't be set for UD/UC in modify_qp */
	rpci_ep_qp_init_resp(rpci, qp, init, udata, uresp);
		if (!qp->srq) {
			qp->rq.max_wr		= init->cap.max_recv_wr;
			qp->rq.max_sge		= init->cap.max_recv_sge;
			wqe_size = rcv_wqe_size(qp->rq.max_sge);
			qp->rq.queue = rpci_ep_queue_init(rpci, &qp->rq.max_wr, wqe_size);
			ep_do_mmap_info(rpci, uresp ? &uresp->rq_mi : NULL, udata, qp->rq.queue->buf, qp->rq.queue->buf_size, &qp->rq.queue->ip);
		}
		rpci_ep_init_task(rpci, &qp->resp.task, qp, rpci_ep_responder, "resp");
		qp->resp.opcode		= OPCODE_NONE;
		qp->resp.msn		= 0;
		qp->resp.state		= QP_STATE_RESET;
	qp->attr.qp_state = IB_QPS_RESET;
	qp->valid = 1;


int rpci_ep_requester(void *arg)
	struct rpci_ep_qp *qp = (struct rpci_ep_qp *)arg;
next_wqe:
	wqe = req_next_wqe(qp);
		struct rxe_send_wqe *wqe = queue_head(qp->sq.queue);
			return q->buf->data + ((q->buf->consumer_index & q->index_mask) << q->log2_elem_size);
		wqe = addr_from_index(qp->sq.queue, qp->req.wqe_index);
			return q->buf->data + ((index & q->index_mask) << q->buf->log2_elem_size);
		wqe->mask = wr_opcode_mask(wqe->wr.opcode, qp);
	if (wqe->mask & WR_REG_MASK) {
		if (wqe->wr.opcode == IB_WR_LOCAL_INV) {
			struct rpci_ep_dev *rpci = to_rdev(qp->ibqp.device);
			struct rpci_ep_mem *rmr;
			rmr = rpci_ep_pool_get_index(&rpci->mr_pool, wqe->wr.ex.invalidate_rkey >> 8);
			rmr->state = RPCI_MEM_STATE_FREE;
			wqe->state = wqe_state_done;
			wqe->status = IB_WC_SUCCESS;
		} else if (wqe->wr.opcode == IB_WR_REG_MR) {
			struct rpci_ep_mem *rmr = to_rmr(wqe->wr.wr.reg.mr);
			rmr->state = RPCI_MEM_STATE_VALID;
			rmr->access = wqe->wr.wr.reg.access;
			rmr->lkey = wqe->wr.wr.reg.key;
			rmr->rkey = wqe->wr.wr.reg.key;
			rmr->iova = wqe->wr.wr.reg.mr->iova;
			wqe->state = wqe_state_done;
			wqe->status = IB_WC_SUCCESS;
		}
		if ((wqe->wr.send_flags & IB_SEND_SIGNALED) || qp->sq_sig_type == IB_SIGNAL_ALL_WR)
			rpci_ep_run_task(&qp->comp.task, 1);
	}
	opcode = next_opcode(qp, wqe, wqe->wr.opcode);
	mask = rpci_ep_opcode[opcode].mask;
	mtu = get_mtu(qp);
	payload = (mask & RPCI_WRITE_OR_SEND) ? wqe->dma.resid : 0;
	skb = init_req_packet(qp, wqe, opcode, payload, &pkt);
	fill_packet(qp, wqe, &pkt, skb, payload)
		struct rpci_ep_dev *rpci = to_rdev(qp->ibqp.device);
		rpci_ep_prepare(pkt, skb, &crc);
		if (pkt->mask & RPCI_WRITE_OR_SEND) {
			if (wqe->wr.send_flags & IB_SEND_INLINE) {
				u8 *tmp = &wqe->dma.inline_data[wqe->dma.sge_offset];
				crc = rpci_crc32(rpci, crc, tmp, paylen);
				memcpy(payload_addr(pkt), tmp, paylen);
				wqe->dma.resid -= paylen;
				wqe->dma.sge_offset += paylen;
			} else {
				err = ep_copy_data(qp->pd, 0, &wqe->dma, payload_addr(pkt), paylen, from_mem_obj, &crc);
					struct rxe_sge		*sge	= &dma->sge[dma->cur_sge];
					int			offset	= dma->sge_offset;
					if (sge->length && (offset < sge->length))
						mem = ep_lookup_mem(pd, access, sge->lkey, lookup_local);
					while (length > 0) {
						bytes = length;
						if (offset >= sge->length) {
							sge++;
							dma->cur_sge++;
							offset = 0;
							if (sge->length) {
								mem = ep_lookup_mem(pd, access, sge->lkey, lookup_local);
							} else {
								continue;
							}
						}
						if (bytes > sge->length - offset) bytes = sge->length - offset;
						if (bytes > 0) {
							iova = sge->addr + offset;
							err = rpci_ep_mem_copy(mem, iova, addr, bytes, dir, crcp);
								if (mem->type == RPCI_MEM_TYPE_DMA) {
									u8 *src, *dest;
									src  = (dir == to_mem_obj) ?  addr : ((void *)(uintptr_t)iova);
									dest = (dir == to_mem_obj) ?  ((void *)(uintptr_t)iova) : addr;
									memcpy(dest, src, length);
							offset	+= bytes;
							resid	-= bytes;
							length	-= bytes;
							addr	+= bytes;
						}
					}
					dma->sge_offset = offset;
					dma->resid	= resid;
			}
		}
		p = payload_addr(pkt) + paylen + bth_pad(pkt);
		*p = ~crc;
	save_state(wqe, qp, &rollback_wqe, &rollback_psn);
	update_wqe_state(qp, wqe, &pkt);
	update_wqe_psn(qp, wqe, &pkt, payload);
	rpci_ep_xmit_packet(qp, &pkt, skb);
	update_state(qp, wqe, &pkt, payload);


/* implements a simple circular buffer that can optionally be
 * shared between user space and the kernel and can be resized

 * the requested element size is rounded up to a power of 2
 * and the number of elements in the buffer is also rounded
 * up to a power of 2. Since the queue is empty when the
 * producer and consumer indices match the maximum capacity
 * of the queue is one less than the number of element slots
 */

/* this data structure is shared between user space and kernel
 * space for those cases where the queue is shared. It contains
 * the producer and consumer indices. Is also contains a copy
 * of the queue size parameters for user space to use but the
 * kernel must use the parameters in the rpci_queue struct
 * this MUST MATCH the corresponding librpci struct
 * for performance reasons arrange to have producer and consumer
 * pointers in separate cache lines
 * the kernel should always mask the indices to avoid accessing
 * memory outside of the data area
 */
struct rpci_queue_buf {
	__u32			log2_elem_size;
	__u32			index_mask;
	__u32			pad_1[30];
	__u32			producer_index;
	__u32			pad_2[31];
	__u32			consumer_index;
	__u32			pad_3[31];
	__u8			data[0];
};

struct rpci_ep_qp {
	struct rpci_ep_sq		sq;  ---------------------> struct rpci_ep_sq {
	struct rpci_ep_rq		rq;                               struct rpci_queue	*queue; --------------------> struct rpci_queue {
	struct rpci_ep_dev		*rpci;                      }                                                               struct rpci_queue_buf *buf;
};                                                                                                                             }


##################################
# /dev/infiniband/uverb????????????????
##################################

enum ib_uverbs_write_cmds {
/*0-4*/IB_USER_VERBS_CMD_GET_CONTEXT, IB_USER_VERBS_CMD_QUERY_DEVICE, IB_USER_VERBS_CMD_QUERY_PORT, IB_USER_VERBS_CMD_ALLOC_PD, IB_USER_VERBS_CMD_DEALLOC_PD, 
/*5-9*/IB_USER_VERBS_CMD_CREATE_AH, IB_USER_VERBS_CMD_MODIFY_AH, IB_USER_VERBS_CMD_QUERY_AH, IB_USER_VERBS_CMD_DESTROY_AH, IB_USER_VERBS_CMD_REG_MR,
/*10-14*/IB_USER_VERBS_CMD_REG_SMR, IB_USER_VERBS_CMD_REREG_MR, IB_USER_VERBS_CMD_QUERY_MR, IB_USER_VERBS_CMD_DEREG_MR, IB_USER_VERBS_CMD_ALLOC_MW,
/*15-19*/IB_USER_VERBS_CMD_BIND_MW, IB_USER_VERBS_CMD_DEALLOC_MW, IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL, IB_USER_VERBS_CMD_CREATE_CQ, IB_USER_VERBS_CMD_RESIZE_CQ,
/*20-24*/IB_USER_VERBS_CMD_DESTROY_CQ, IB_USER_VERBS_CMD_POLL_CQ, IB_USER_VERBS_CMD_PEEK_CQ, IB_USER_VERBS_CMD_REQ_NOTIFY_CQ, IB_USER_VERBS_CMD_CREATE_QP,
/*25-29*/IB_USER_VERBS_CMD_QUERY_QP, IB_USER_VERBS_CMD_MODIFY_QP, IB_USER_VERBS_CMD_DESTROY_QP, IB_USER_VERBS_CMD_POST_SEND, IB_USER_VERBS_CMD_POST_RECV,
/*30-34*/IB_USER_VERBS_CMD_ATTACH_MCAST, IB_USER_VERBS_CMD_DETACH_MCAST, IB_USER_VERBS_CMD_CREATE_SRQ, IB_USER_VERBS_CMD_MODIFY_SRQ, IB_USER_VERBS_CMD_QUERY_SRQ,
/*35-39*/IB_USER_VERBS_CMD_DESTROY_SRQ, IB_USER_VERBS_CMD_POST_SRQ_RECV, IB_USER_VERBS_CMD_OPEN_XRCD, IB_USER_VERBS_CMD_CLOSE_XRCD, IB_USER_VERBS_CMD_CREATE_XSRQ,
/*40*/IB_USER_VERBS_CMD_OPEN_QP,
};

struct uapi_definition {
	u8 kind;
	u8 scope;
	union {
		struct { u16 object_id; } object_start;
		struct {
			u16 command_num;
			u8 is_ex:1;
			u8 has_udata:1;
			u8 has_resp:1;
			u8 req_size;
			u8 resp_size;
		} write;
	};

	union {
		bool (*func_is_supported)(struct ib_device *device);
		int (*func_write)(struct uverbs_attr_bundle *attrs);
		const struct uapi_definition *chain;
		const struct uverbs_object_def *chain_obj_tree;
		size_t needs_fn_offset;
	};
};

const struct uapi_definition uverbs_def_write_intf[] = {
{
	.kind = UAPI_DEF_OBJECT_START,
	.object_start = { .object_id = UVERBS_OBJECT_AH},
	},
	{
	.kind = UAPI_DEF_WRITE,
	.scope = UAPI_SCOPE_OBJECT,
	.write = { .is_ex = 0, .command_num = IB_USER_VERBS_CMD_CREATE_AH},
	.func_write = ib_uverbs_create_ah,
	.write.has_udata = 1+ ?
	},
	{
	.kind = UAPI_DEF_WRITE,
	.scope = UAPI_SCOPE_OBJECT,
	.write = { .is_ex = 0, .command_num = IB_USER_VERBS_CMD_DESTROY_AH},
	.func_write = ib_uverbs_destroy_ah,
	.write.req_size = sizeof(struct ib_uvers_destroy_ah)
	.write.has_udata = 1+ ?
	},
	{
	.kind = UAPI_DEF_IS_SUPPORTED_DEV_FN,
	.scope = UAPI_SCOPE_METHOD,
	.needs_fn_offset = offsetof(struct ib_device_ops, destroy_ah) + BUILD_BUG_ON_ZERO(sizeof(((struct ib_device_ops *)0)->destroy_ah) != sizeof(void *)),
	}
	,????
}

static const struct file_operations uverbs_mmap_fops = {
	.owner   = THIS_MODULE,
	.write   = ib_uverbs_write,
	.mmap    = ib_uverbs_mmap,
	.open    = ib_uverbs_open,
	.release = ib_uverbs_close,
	.llseek  = no_llseek,
	.unlocked_ioctl = ib_uverbs_ioctl,
	.compat_ioctl = ib_uverbs_ioctl,
};

static struct ib_client uverbs_client = {
	.name   = "uverbs",
	.no_kverbs_req = true,
	.add    = ib_uverbs_add_one,
	.remove = ib_uverbs_remove_one,
	.get_nl_info = ib_uverbs_get_nl_info,
};
MODULE_ALIAS_RDMA_CLIENT("uverbs");

ib_uvers_init() // module_init(ib_uverbs_init);
> register_chrdev_region(IB_UVERBS_BASE_DEV, IB_UVERBS_NUM_FIXED_MINOR, "infiniband_verbs");
> alloc_chrdev_region(&dynamic_uverbs_dev, 0, IB_UVERBS_NUM_DYNAMIC_MINOR, "infiniband_verbs");
> uverbs_class = class_create(THIS_MODULE, "infiniband_verbs");
> uverbs_class->devnode = uverbs_devnode;
> class_create_file(uverbs_class, &class_attr_abi_version.attr);
> ib_register_client(&uverbs_client);

ib_uverbs_add_one()
> uverbs_dev = kzalloc(sizeof(*uverbs_dev), GFP_KERNEL);
> init_srcu_struct(&uverbs_dev->disassociate_srcu);
> device_initialize(&uverbs_dev->dev);
> uverbs_dev->dev.class = uverbs_class; uverbs_dev->dev.parent = device->dev.parent; uverbs_dev->dev.release = ib_uverbs_release_dev; uverbs_dev->dev.groups = uverbs_dev->groups;
> uverbs_dev->groups[0] = &dev_attr_group;
> atomic_set(&uverbs_dev->refcount, 1);
> init_completion(&uverbs_dev->comp);
> uverbs_dev->xrcd_tree = RB_ROOT;
> mutex_init(&uverbs_dev->xrcd_tree_mutex); mutex_init(&uverbs_dev->lists_mutex);
> INIT_LIST_HEAD(&uverbs_dev->uverbs_file_list); INIT_LIST_HEAD(&uverbs_dev->uverbs_events_file_list);
> rcu_assign_pointer(uverbs_dev->ib_dev, device);
> uverbs_dev->num_comp_vectors = device->num_comp_vectors;
> devnum = ida_alloc_max(&uverbs_ida, IB_UVERBS_MAX_DEVICES - 1, GFP_KERNEL);
> uverbs_dev->devnum = devnum;
> if (devnum >= IB_UVERBS_NUM_FIXED_MINOR) base = dynamic_uverbs_dev + devnum - IB_UVERBS_NUM_FIXED_MINOR;
> else base = IB_UVERBS_BASE_DEV + devnum;
> ib_uverbs_create_uapi(device, uverbs_dev)
> uverbs_dev->dev.devt = base;
> dev_set_name(&uverbs_dev->dev, "uverbs%d", uverbs_dev->devnum);
> cdev_init(&uverbs_dev->cdev, device->ops.mmap ? &uverbs_mmap_fops : &uverbs_fops);
> uverbs_dev->cdev.owner = THIS_MODULE;
> cdev_device_add(&uverbs_dev->cdev, &uverbs_dev->dev);
> ib_set_client_data(device, &uverbs_client, uverbs_dev);

ib_uverbs_open()
> dev = container_of(inode->i_cdev, struct ib_uverbs_device, cdev);
> ib_dev = srcu_dereference(dev->ib_dev, &dev->disassociate_srcu);
> file = kzalloc(sizeof(*file), GFP_KERNEL);
> file->device	 = dev;
> filp->private_data = file;
> list_add_tail(&file->list, &dev->uverbs_file_list);
> return stream_open(inode, filp);

ib_uverbs_write()
> copy_from_user(&hdr, buf, sizeof(hdr))
> method_elm = uapi_get_method(uapi, hdr.command);
> verify_hdr(&hdr, &ex_hdr, count, method_elm);
> srcu_key = srcu_read_lock(&file->device->disassociate_srcu);
> buf += sizeof(hdr);
> memset(bundle.attr_present, 0, sizeof(bundle.attr_present));
> bundle.ufile = file; bundle.context = NULL;
> if (!method_elm->is_ex) {
>> set bundle.driver_udata
>> ib_uverbs_init_udata_buf_or_null( &bundle.ucore, buf, u64_to_user_ptr(response), in_len, out_len);
> else
>> buf += sizeof(ex_hdr);
>> ib_uverbs_init_udata_buf_or_null(&bundle.ucore, buf, u64_to_user_ptr(ex_hdr.response), hdr.in_words * 8, hdr.out_words * 8);
>> ib_uverbs_init_udata_buf_or_null( &bundle.driver_udata, buf + bundle.ucore.inlen, u64_to_user_ptr(ex_hdr.response) + bundle.ucore.outlen, ex_hdr.provider_in_words * 8, ex_hdr.provider_out_words * 8);
> method_elm->handler(&bundle);

ib_uvebrs_ioctl()
> struct ib_uverbs_file *file = filp->private_data;
> struct ib_uverbs_ioctl_hdr __user *user_hdr = (struct ib_uverbs_ioctl_hdr __user *)arg;
> copy_from_user(&hdr, user_hdr, sizeof(hdr));
> ib_uverbs_cmd_verbs(file, &hdr, user_hdr->attrs);

ib_uverbs_mmap()
> struct ib_uverbs_file *file = filp->private_data;
> ucontext = ib_uverbs_get_ucontext_file(file);
> ucontext->device->ops.mmap(ucontext, vma);
